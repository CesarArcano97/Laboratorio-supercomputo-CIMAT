# Uso del Laboratorio de SupercÃ³mputo CIMAT

## Acceso y prueba inicial en el Laboratorio de SupercÃ³mputo del BajÃ­o (CIMAT)

Este documento registra los pasos realizados para conectarnos y validar el uso de GPUs (NVIDIA TITAN RTX) en el cluster **Lab-SB** de CIMAT. ServirÃ¡ como guÃ­a de referencia para futuros proyectos.

---

## Acceso vÃ­a SSH (desde Linux)

1. Conectarse al **front-end** del cluster:
   ```bash
   ssh -p <port> <username>@<host>

* \<port\> â†’ puerto asignado en correo de alta.

* \<username\> â†’ tu usuario (ejemplo: est_posgrado_cesar.aguirre).

* \<host\> â†’ direcciÃ³n del cluster (ejemplo: el-insurgente.cimat.mx).

### Verificamos el directorio home:

```bash
pwd
ls
```

## RevisiÃ³n de particiones disponibles de GPU

Al consultar ```sinfo``` se identificaron las particiones con GPUs:

```bash
sinfo -o "%P %G %N" | grep GPU
```

Salida:

```scss
GPU (null) g-0-[1-12]
GPUX (null) g-0-[1-12]
GPU_PROY2025 (null) g-0-[4-9]
GPU_WS (null) g-ws-[1-3] 
```

Esto confirma que los nodos ```g-0-x``` tienen 2x NVIDIA TITAN RTX (24 GB) cada uno.


## Prueba inicial con Slurm 

Se crea un archivo de ```gpu_test.slurm```:

```bash
#!/bin/bash
#SBATCH --job-name=gpu_test
#SBATCH --partition=GPU
#SBATCH --time=00:05:00
#SBATCH --output=gpu_test_%j.out

# Validar GPU disponible
nvidia-smi
```

Ejecutar:

```bash
sbatch gpu_test.slurm
squeue -u $USER
```

Revisar el log::

```bash
cat gpu_test_<jobid>.out
```

El resultado debe ser algo del estilo:

```lua
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08   Driver Version: 545.23.08   CUDA Version: 12.3                 |
| GPU  Name          Persistence-M | Bus-Id | Temp | Memory-Usage | GPU-Util | Compute M|
|  0  NVIDIA TITAN RTX            ...       | 24 GB |              |         |          |
|  1  NVIDIA TITAN RTX            ...       | 24 GB |              |         |          |
+---------------------------------------------------------------------------------------+
| No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

Esto confirma el acceso exitoso a los nodos de la GPU, y su disponibilidad. AdemÃ¡s de la versiÃ³n de CUDA preparada para usarse. 


## InstalaciÃ³n de Miniconda en $HOME

```bash
cd $HOME
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
bash miniconda.sh -b -p $HOME/miniconda3
echo 'export PATH=$HOME/miniconda3/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
```

InicializaciÃ³n de conda:

```bash
conda init bash
source ~/.bashrc
```

## Entorno Conda para Deep Learning

CreaciÃ³n del entorno ```prometheus```:

```bash
conda create -n prometheus python=3.11 -y
conda activate prometheus
```

InstalaciÃ³n de PyTorch con soporte GPU (CUDA 12.1):

```bash
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

## Prueba de PyTorch + GPU en Slurm:

```bash
#!/bin/bash
#SBATCH --job-name=torch_gpu_test
#SBATCH --partition=GPU
#SBATCH --time=00:05:00
#SBATCH --output=torch_gpu_test_%j.out

# Inicializar conda en nodos Slurm
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"
conda activate prometheus

python - << 'EOF'
import torch
print("PyTorch versiÃ³n:", torch.__version__)
print("CUDA disponible:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU detectada:", torch.cuda.get_device_name(0))
    print("VersiÃ³n CUDA en PyTorch:", torch.version.cuda)
EOF
```

Ejecutar:

```bash
sbatch torch_gpu_test.slurm
```

Salida esperada:

```bash
PyTorch versiÃ³n: 2.5.1
CUDA disponible: True
GPU detectada: NVIDIA TITAN RTX
VersiÃ³n CUDA en PyTorch: 12.1
```

<h2> EjecuciÃ³n exitosa en el clÃºster Lab-SB (CIMAT)</h2>

<p>Se logrÃ³ entrenar y ejecutar un modelo <code>tiny-gpt2</code> en el laboratorio de supercÃ³mputo de CIMAT, siguiendo buenas prÃ¡cticas de organizaciÃ³n y uso de SLURM.</p>

<h3> Estructura del proyecto</h3>

<pre>
proyecto_gpt2/
â”œâ”€â”€ data/               # Corpus de entrenamiento
â”‚   â””â”€â”€ TOP_corpus_generativo_unificado.txt
â”œâ”€â”€ src/                # CÃ³digo fuente
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ models/             # Modelos locales y checkpoints
â”‚   â””â”€â”€ tiny-gpt2/      # Modelo predescargado desde Hugging Face
â”œâ”€â”€ logs/               # Logs de SLURM
â”‚   â””â”€â”€ gpt2_finetune-<JOBID>.log
â”œâ”€â”€ results/            # MÃ©tricas y outputs
â””â”€â”€ run_entrenamiento.sh # Script de lanzamiento con SLURM
</pre>

<h3> Pasos realizados</h3>

<ol>
  <li>Crear un entorno <code>conda</code> especÃ­fico para NLP/Transformers:
    <pre>conda create -n prometheus python=3.11 -y</pre>
  </li>
  <li>Instalar dependencias necesarias:
    <pre>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
pip install transformers datasets protobuf</pre>
  </li>
  <li>Descargar el modelo <code>tiny-gpt2</code> en local y subirlo al clÃºster (ya que los nodos no tienen internet).</li>
  <li>Colocar el corpus en <code>data/TOP_corpus_generativo_unificado.txt</code>.</li>
  <li>Lanzar entrenamiento de prueba en CPU desde la terminal:
    <pre>
python src/train.py \
  --model_name "./models/tiny-gpt2" \
  --train_file "./data/TOP_corpus_generativo_unificado.txt" \
  --epochs 2
    </pre>
  </li>
</ol>

<h3> Resultados</h3>

<ul>
  <li>Entrenamiento completado exitosamente (2 Ã©pocas, 100 steps).</li>
  <li>PÃ©rdida final aproximada: <code>train_loss â‰ˆ 10.74</code>.</li>
  <li>Modelo guardado en: <code>./models/final</code>.</li>
  <li>GeneraciÃ³n de texto con el prompt inicial funcionando.</li>
</ul>

<h3> Cosas a tomar en cuenta</h3>

<ul>
  <li>Los nodos del clÃºster <b>no tienen internet</b>, por lo que los modelos/tokenizers deben descargarse previamente y transferirse completos.</li>
  <li>Es esencial mantener una <b>estructura de proyecto organizada</b> (data, src, models, logs, results).</li>
  <li>El uso de <code>run_entrenamiento.sh</code> con <code>sbatch</code> facilita reproducir experimentos en GPU.</li>
  <li>Los logs de SLURM (<code>logs/gpt2_finetune-*.log</code>) son la primera fuente para depuraciÃ³n.</li>
</ul>

_________________________________________________________

# Proyecto llama-beta

Este documento recopila los pasos que funcionaron en el **cluster Lab-SB de CIMAT** para afinar el modelo `TinyLLaMA` sobre un corpus propio (canciones de TOP).

---

## Estructura del proyecto

```arduino
llama-beta/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ TOP_corpus_generativo_unificado.txt
â”œâ”€â”€ logs/ # logs de SLURM
â”œâ”€â”€ models/
â”‚ â””â”€â”€ tiny-llama-1b/ # modelo predescargado desde laptop
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ run_train.sh # script de entrenamiento con SLURM
â””â”€â”€ src/
â””â”€â”€ train.py # script de fine-tuning
```

### Entrenamiento en el cluster

Se lanza el entrenamiento con SLURM:

```bash
cd ~/llama-beta
sbatch scripts/run_train.sh
```

Revisar si el job sigue en cola o ya corriendo:

```bash
squeue -u $USER
```

Monitoreo y logs

Para ver el log en vivo:

```bash
tail -f logs/llama_beta_train-<JOBID>.log
```

Al terminar, puedes descargar el log a tu PC (corriendo desde el server):

```bash
ssh <USUARIO>@el-insurgente.cimat.mx "cat ~/llama-beta/logs/llama_beta_train-<JOBID>.log" > llama_beta_train-<JOBID>.log
```

### Puedes aÃ±adir notificaciones para ver cuando termine tu log

Agregando al script de train.sh:

```bash
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=tu_correo@cimat.mx
```

O bien un monitoreo manual:
```bash
#!/usr/bin/env bash
JOBID="$1"
[ -z "$JOBID" ] && { echo "Uso: $0 <JOBID>"; exit 1; }

echo "Esperando a que termine el job $JOBID..."
while squeue -j "$JOBID" -h >/dev/null 2>&1; do sleep 60; done

STATE=$(sacct -j "$JOBID" --format=State --noheader | head -n1 | awk '{print $1}')
[ -z "$STATE" ] && STATE="FINISHED"
echo -e "Job $JOBID terminÃ³ con estado: $STATE\a"
```

Uso: `scripts/notify_when_done.sh <JOBID>`

### Descarga de datos cluster a mÃ¡quina externa:

## Descargar resultados a mi PC (fuera de CIMAT)

Cuando se estÃ¡ fuera de CIMAT, se debe usar el **host** indicado y **puerto** indicado en el registro.

#### OpciÃ³n rÃ¡pida (`scp`)
> Nota: la `-P` de puerto es **mayÃºscula**.
```bash
scp -P <PUERTO> -r \
  <USUARIO>@<HOST>:~/DIRECTORIO-DE-INTERES \
  /home/USUARIO/
```
# Fine-tuning y GeneraciÃ³n Offline con Mistral-7B + Unsloth (LoRA)

Este flujo documenta los pasos funcionales para entrenar y generar letras de canciones **offline** en el servidor **el-insurgente (CIMAT)** utilizando **Mistral-7B-Instruct** con **LoRA eficiente (Unsloth)**.

---

## 1. Estructura del proyecto

```
mistral-project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ lyrics_train.jsonl          â† corpus a nivel texto
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â””â”€â”€ Mistral-7B-Instruct-v0.2    â† modelo base descargado
â”‚   â””â”€â”€ finetuned/
â”‚       â””â”€â”€ t21p_lr2e-4_ep3_bs2x4_YYYYMMDD_HHMMSS/
â”‚           â””â”€â”€ final_model/            â† adaptadores LoRA + tokenizer
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ generations/                    â† letras generadas
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ train_.out, generate_.out
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ train/train_mistral_offline.py
    â””â”€â”€ generate/generate_mistral_offline.py
```

---

## 2. Entorno Conda funcional

```bash
conda create -n mistral-env python=3.10 -y
conda activate mistral-env

pip install torch==2.8.0 \
            transformers==4.51.3 \
            accelerate==1.1.1 \
            trl==0.9.4 \
            peft==0.11.1 \
            unsloth==2025.9.7 \
            bitsandbytes==0.43.1 \
            xformers==0.0.32.post2 \
            datasets==2.21.0
```
**VersiÃ³n estable confirmada:**
- **Torch 2.8.0** + **CUDA 12.8**
- **Unsloth 2025.9.7** compatible con Mistral 7B.

---

## 3. Entrenamiento offline con LoRA

**Archivo:** `src/train/train_mistral_offline.py`

**Puntos clave:**

- **Modo totalmente offline** configurado con:
  ```python
  import os
  os.environ["HF_HUB_OFFLINE"] = "1"
  os.environ["TRANSFORMERS_OFFLINE"] = "1"
  os.environ["HF_DATASETS_OFFLINE"] = "1"
  os.environ["UNSLOTH_FORCE_OFFLINE"] = "1"
  ```
- **Carga local** desde: `~/mistral-project/models/base/Mistral-7B-Instruct-v0.2`
- **LoRA** aplicada sÃ³lo en proyecciones Q/K/V/O (â‰ˆ 1 % de parÃ¡metros entrenados).
- **Gradiente acumulado** para lotes pequeÃ±os (`grad_accum=4`).
- **Guardado automÃ¡tico** de adaptadores + tokenizer en: `models/finetuned/<run_name>/final_model/`

**Ejemplo de comando Slurm:**
```bash
#!/bin/bash
#SBATCH --job-name=train_mistral
#SBATCH --partition=GPU
#SBATCH --gres=gpu:1
#SBATCH --time=02:00:00
#SBATCH --output=logs/train_%j.out
#SBATCH --error=logs/train_%j.err

source ~/miniconda3/etc/profile.d/conda.sh
conda activate mistral-env
cd ~/mistral-project/src/train

python train_mistral_offline.py
```

**Resultado exitoso:**
```bash
Trainable parameters = 13.6M (0.19% of total)
train_loss â‰ˆ 1.59
Modelo guardado en: models/finetuned/.../final_model/
```

---

## ğŸ¤ 4. GeneraciÃ³n offline

**Archivo:** `src/generate/generate_mistral_offline.py`

- Combina modelo base + adaptadores LoRA vÃ­a `PeftModel.from_pretrained`.
- Usa `device_map="auto"` (carga en GPU automÃ¡ticamente).
- **Prompts configurables:**
  ```python
  prompts = [
      "In the city lights I find myself",
      "They say I'm broken but I'm breathing",
      "Sometimes my shadow sings louder than I do"
  ]
  ```
- **ParÃ¡metros de sampling:**
  ```python
  max_new_tokens=220, temperature=0.9, top_p=0.95, do_sample=True
  ```
- Letras generadas en: `results/generations/song_1.txt`, `song_2.txt`, etc.

**Ejemplo de Slurm de inferencia:**
```bash
#!/bin/bash
#SBATCH --job-name=generate_mistral
#SBATCH --partition=GPU
#SBATCH --gres=gpu:1
#SBATCH --time=00:10:00
#SBATCH --output=logs/generate_%j.out
#SBATCH --error=logs/generate_%j.err

source ~/miniconda3/etc/profile.d/conda.sh
conda activate mistral-env
cd ~/mistral-project/src/generate

python generate_mistral_offline.py
```
**Salida esperada:**
```bash
ğŸ”¹ Cargando modelo base en GPU...
ğŸ”¹ Cargando adaptadores LoRA...
Generando canciÃ³n 1...
Guardada: results/generations/song_1.txt
...
```

---

## 5. Ejemplo de letra generada

**Prompt:** â€œIn the city lights I find myselfâ€

```
In the city lights I find myself
I don't know why
I wanna stay inside tonight
I think it's right
But my heart keeps telling me that I should go...
```
Original, coherente y estilÃ­sticamente alineada con Twenty One Pilots.
Sin coincidencias literales con letras oficiales (verificado manualmente).

---

## 6. Notas finales

- **Unsloth con LoRA** ofrece â‰ˆ **2Ã— menor VRAM** y entrenamiento estable en 1 GPU TITAN RTX (24 GB).
- Todos los procesos se ejecutaron **offline**, sin conexiÃ³n a Hugging Face.
- La ruta mÃ¡s reciente del modelo entrenado:
  ```bash
  ~/mistral-project/models/finetuned/t21p_lr2e-4_ep3_bs2x4_20251004_033651/final_model
  ```

**Estado actual del pipeline:** Fine-tuning completo + generaciÃ³n funcional offline.
